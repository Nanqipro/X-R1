#!/bin/bash

# This script is generated by AI to address the CUDA out of memory issue.
# It separates the vLLM inference engine onto a dedicated GPU (cuda:3)
# and uses the other GPUs (cuda:0,1,2) for training.

# Stop on first error
set -e

# Make all 4 GPUs visible to the script
export CUDA_VISIBLE_DEVICES=0,1,2,3

# Get the directory of the current script
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_ROOT="$SCRIPT_DIR" 

# Path to the training script
TRAINING_SCRIPT="$PROJECT_ROOT/src/x_r1/grpo.py"

# DeepSpeed configuration file
DEEPSPEED_CONFIG="$PROJECT_ROOT/recipes/zero3.yaml"

# Set number of processes for training
NUM_PROCESSES=3

# Set a random free port for the main process
MAIN_PROCESS_PORT=$(( RANDOM % 20000 + 20000 ))

echo "Starting training on GPUs 0,1,2 and vLLM on GPU 3..."
echo "Using port $MAIN_PROCESS_PORT for main process..."

# Launch training with accelerate
# --vllm_device "cuda:3" is added to explicitly tell the script to use GPU 3 for vLLM.
# The training processes launched by accelerate will use the first N GPUs available based on its logic (0, 1, 2).
accelerate launch --config_file "$PROJECT_ROOT/recipes/zero3.yaml" --num_processes $NUM_PROCESSES --main_process_port $MAIN_PROCESS_PORT "$TRAINING_SCRIPT" \
    --deepspeed "$DEEPSPEED_CONFIG" \
    --model_name_or_path "../LLM-models-datasets/Qwen2.5-3B" \
    --use_peft True \
    --lora_r 16 \
    --lora_alpha 16 \
    --lora_target_modules "q_proj" "v_proj" \
    --dataset_name "../LLM-models-datasets/Bespoke-Stratos-17k" \
    --dataset_config "default" \
    --dataset_train_split "train" \
    --dataset_test_split "test" \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 3 \
    --learning_rate 3e-6 \
    --lr_scheduler_type "cosine" \
    --warmup_ratio 0.1 \
    --logging_steps 5 \
    --save_strategy "epoch" \
    --output_dir "output/X-R1-3B-LoRA-Advanced" \
    --overwrite_output_dir \
    --max_prompt_length 256 \
    --max_completion_length 256 \
    --num_generations 3 \
    --beta 0.04 \
    --loss_type "bnpo" \
    --gradient_checkpointing True \
    --fp16 True \
    --use_vllm True \
    --vllm_gpu_memory_utilization 0.3 \
    --vllm_device "cuda:3" \
    --script_num_iterations 1 \
    --reward_funcs "accuracy_continuous" "format_continuous" "reasoning_steps" \
    --cosine_min_value_wrong 0.0 \
    --cosine_max_value_wrong -0.5 \
    --cosine_min_value_correct 0.5 \
    --cosine_max_value_correct 1.0 \
    --cosine_max_len 1000 \
    --repetition_n_grams 3 \
    --repetition_max_penalty -1.0 