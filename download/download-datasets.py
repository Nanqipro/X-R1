#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MMLUæ•°æ®é›†ä¸‹è½½å™¨

è¯¥è„šæœ¬ç”¨äºä»Hugging Faceä¸‹è½½cais/mmluæ•°æ®é›†åˆ°æœ¬åœ°æŒ‡å®šç›®å½•
"""

import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any
from datasets import load_dataset, Dataset
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def setup_download_directory(base_path: str) -> Path:
    """
    è®¾ç½®å¹¶åˆ›å»ºä¸‹è½½ç›®å½•
    
    Parameters
    ----------
    base_path : str
        åŸºç¡€è·¯å¾„å­—ç¬¦ä¸²
        
    Returns
    -------
    Path
        åˆ›å»ºçš„ç›®å½•è·¯å¾„å¯¹è±¡
    """
    download_dir = Path(base_path)
    download_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"ä¸‹è½½ç›®å½•å·²å‡†å¤‡: {download_dir}")
    return download_dir


def download_mmlu_dataset(
    dataset_name: str = "cais/mmlu", 
    config_name: str = "all",
    cache_dir: Optional[str] = None,
    trust_remote_code: bool = True
) -> Dict[str, Dataset]:
    """
    ä¸‹è½½MMLUæ•°æ®é›†
    
    Parameters
    ----------
    dataset_name : str, optional
        æ•°æ®é›†åç§°ï¼Œé»˜è®¤ä¸º "cais/mmlu"
    config_name : str, optional
        é…ç½®åç§°ï¼Œé»˜è®¤ä¸º "all" (åŒ…å«æ‰€æœ‰å­¦ç§‘)
    cache_dir : str, optional
        ç¼“å­˜ç›®å½•è·¯å¾„
    trust_remote_code : bool, optional
        æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼Œé»˜è®¤ä¸ºTrue
        
    Returns
    -------
    Dict[str, Dataset]
        åŒ…å«å„ä¸ªåˆ†å‰²çš„æ•°æ®é›†å­—å…¸
        
    Raises
    ------
    Exception
        å½“ä¸‹è½½å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    try:
        logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        logger.info(f"é…ç½®: {config_name}")
        logger.info(f"ç›®æ ‡è·¯å¾„: {cache_dir}")
        
        # ä¸‹è½½æ•°æ®é›† (æŒ‡å®šé…ç½®åç§°)
        dataset = load_dataset(
            dataset_name,
            config_name,
            cache_dir=cache_dir,
            trust_remote_code=trust_remote_code
        )
        
        logger.info("æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
        
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        for split_name, split_data in dataset.items():
            logger.info(f"  - {split_name}: {len(split_data)} æ¡è®°å½•")
            
        return dataset
        
    except Exception as e:
        logger.error(f"æ•°æ®é›†ä¸‹è½½å¤±è´¥: {str(e)}")
        raise


def verify_dataset(dataset: Dict[str, Dataset]) -> bool:
    """
    éªŒè¯ä¸‹è½½çš„æ•°æ®é›†å®Œæ•´æ€§
    
    Parameters
    ----------
    dataset : Dict[str, Dataset]
        å·²ä¸‹è½½çš„æ•°æ®é›†
        
    Returns
    -------
    bool
        éªŒè¯æ˜¯å¦é€šè¿‡
    """
    try:
        logger.info("éªŒè¯æ•°æ®é›†å®Œæ•´æ€§...")
        
        if not dataset:
            logger.error("æ•°æ®é›†ä¸ºç©º")
            return False
            
        # æ£€æŸ¥åŸºæœ¬åˆ†å‰²
        expected_splits = ['test', 'validation', 'dev']
        available_splits = list(dataset.keys())
        
        logger.info(f"å¯ç”¨çš„æ•°æ®åˆ†å‰²: {available_splits}")
        
        # æ£€æŸ¥æ¯ä¸ªåˆ†å‰²æ˜¯å¦æœ‰æ•°æ®
        for split_name, split_data in dataset.items():
            if len(split_data) == 0:
                logger.warning(f"åˆ†å‰² {split_name} æ²¡æœ‰æ•°æ®")
                return False
                
        logger.info("æ•°æ®é›†éªŒè¯é€šè¿‡!")
        return True
        
    except Exception as e:
        logger.error(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return False


def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒMMLUæ•°æ®é›†ä¸‹è½½æµç¨‹
    """
    # ç›®æ ‡ä¸‹è½½è·¯å¾„
    target_path = "../LLM-models"
    dataset_name = "cais/mmlu"
    
    try:
        logger.info("=" * 50)
        logger.info("MMLUæ•°æ®é›†ä¸‹è½½å™¨å¯åŠ¨")
        logger.info("=" * 50)
        
        # 1. è®¾ç½®ä¸‹è½½ç›®å½•
        download_dir = setup_download_directory(target_path)
        
        # 2. ä¸‹è½½æ•°æ®é›†
        dataset = download_mmlu_dataset(
            dataset_name=dataset_name,
            config_name="all",  # ä¸‹è½½æ‰€æœ‰å­¦ç§‘çš„å®Œæ•´æ•°æ®é›†
            cache_dir=str(download_dir)
        )
        
        # 3. éªŒè¯æ•°æ®é›†
        if verify_dataset(dataset):
            logger.info("âœ… æ•°æ®é›†ä¸‹è½½å¹¶éªŒè¯æˆåŠŸ!")
            logger.info(f"ğŸ“ ä¿å­˜ä½ç½®: {download_dir}")
        else:
            logger.error("âŒ æ•°æ®é›†éªŒè¯å¤±è´¥!")
            sys.exit(1)
            
        # 4. æ˜¾ç¤ºæ•°æ®é›†æ ·ä¾‹
        logger.info("\nğŸ“‹ æ•°æ®é›†æ ·ä¾‹é¢„è§ˆ:")
        for split_name, split_data in dataset.items():
            if len(split_data) > 0:
                logger.info(f"\n{split_name} åˆ†å‰²æ ·ä¾‹:")
                sample = split_data[0]
                for key, value in sample.items():
                    logger.info(f"  - {key}: {str(value)[:100]}...")
                break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªåˆ†å‰²çš„æ ·ä¾‹
                
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)
    
    logger.info("\nğŸ‰ ä»»åŠ¡å®Œæˆ!")


if __name__ == "__main__":
    main()
